import asyncio
import datetime
import hashlib
import json
import os
import re
from typing import Dict, Any, Union, List
import html

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
# ä½¿ç”¨æ ‡å‡†çš„ NATS åº“ï¼Œæ”¯æŒ JetStream
from nats.aio.client import Client as NATS
from bs4 import BeautifulSoup


def md5(text):
    m = hashlib.md5()
    m.update(text.encode('utf-8'))
    return m.hexdigest()


def get_datas(file_path, json_flag=True, all_flag=False, mode='r'):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
    results = []

    if not os.path.exists(file_path):
        return results

    try:
        with open(file_path, mode, encoding='utf-8') as f:
            for line in f.readlines():
                if json_flag:
                    try:
                        results.append(json.loads(line.strip()))
                    except json.JSONDecodeError:
                        continue
                else:
                    results.append(line.strip())
        if all_flag:
            if json_flag:
                return json.loads(''.join(results))
            else:
                return '\n'.join(results)
        return results
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return results


def save_datas(file_path, datas, json_flag=True, all_flag=False, with_indent=False, mode='w'):
    """ä¿å­˜æ–‡æœ¬æ–‡ä»¶"""
    try:
        with open(file_path, mode, encoding='utf-8') as f:
            if all_flag:
                if json_flag:
                    f.write(json.dumps(datas, ensure_ascii=False, indent= 4 if with_indent else None))
                else:
                    f.write(''.join(datas))
            else:
                for data in datas:
                    if json_flag:
                        f.write(json.dumps(data, ensure_ascii=False) + '\n')
                    else:
                        f.write(data + '\n')
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


class AbstractAICrawler():

    def __init__(self) -> None:
        pass
    def crawl(self):
        raise NotImplementedError()


class AINewsCrawler(AbstractAICrawler):
    def __init__(self, domain) -> None:
        super().__init__()
        self.domain = domain
        self.file_path = f'data/{self.domain}.json'
        self.history = self.init()

    def init(self):
        if not os.path.exists(self.file_path):
            return {}
        try:
            data = get_datas(self.file_path)
            return {ele['id']: ele for ele in data if 'id' in ele}
        except Exception as e:
            print(f"åˆå§‹åŒ–å†å²æ•°æ®å¤±è´¥: {e}")
            return {}

    def save(self, datas: Union[List, Dict]):
        if isinstance(datas, dict):
            datas = [datas]
        self.history.update({ele['id']: ele for ele in datas})
        save_datas(self.file_path, datas=list(self.history.values()))


class FinanceNewsCrawler(AINewsCrawler):

    def __init__(self, domain='') -> None:
        super().__init__(domain)

    def save(self, datas: Union[List, Dict]):
        if isinstance(datas, dict):
            datas = [datas]
        self.history.update({ele['id']: ele for ele in datas})

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯æ–°å»º
        file_exists = os.path.exists(self.file_path)
        mode = 'a' if file_exists else 'w'

        save_datas(self.file_path, datas=datas, mode=mode)
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {self.file_path}ï¼Œæ¨¡å¼: {mode}")

    async def get_last_day_data(self):
        last_day = (datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
        datas = self.init()
        return [v for v in datas.values() if last_day in v.get('date', '')]


class CLSCrawler(FinanceNewsCrawler):
    """è´¢è”ç¤¾æ–°é—»æŠ“å– - æ”¯æŒSPAåº”ç”¨"""

    def __init__(self) -> None:
        self.domain = 'cls'
        super().__init__(self.domain)
        self.url = 'https://www.cls.cn'
        self.nats_client = None
        self.js = None  # JetStream ä¸Šä¸‹æ–‡

    async def connect_nats(self):
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨å¹¶è®¾ç½®JetStream"""
        try:
            self.nats_client = NATS()
            await self.nats_client.connect("nats://localhost:4222")
            print("âœ… å·²è¿æ¥åˆ°NATSæœåŠ¡å™¨")

            # è·å–JetStreamä¸Šä¸‹æ–‡
            self.js = self.nats_client.jetstream()

            # åˆ›å»ºStreamï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            try:
                await self.js.add_stream(
                    name="NEWS_STREAM",
                    subjects=["news.>"],  # åŒ¹é…æ‰€æœ‰ä»¥news.å¼€å¤´çš„ä¸»é¢˜
                    retention="limits",   # åŸºäºé™åˆ¶çš„ä¿ç•™ç­–ç•¥
                    max_msgs=10000,      # æœ€å¤§æ¶ˆæ¯æ•°
                    max_bytes=100*1024*1024,  # æœ€å¤§å­—èŠ‚æ•° 100MB
                    max_age=7*24*3600    # ä¿ç•™7å¤©
                )
                print("âœ… åˆ›å»ºäº†JetStreamæµ: NEWS_STREAM")
            except Exception as e:
                if "stream name already in use" in str(e):
                    print("ğŸ“‹ JetStreamæµ NEWS_STREAM å·²å­˜åœ¨")
                else:
                    print(f"âš ï¸ åˆ›å»ºJetStreamæµæ—¶å‡ºé”™: {e}")

        except Exception as e:
            print(f"âŒ è¿æ¥NATSå¤±è´¥: {e}")
            self.nats_client = None
            self.js = None

    async def publish_news(self, news_data):
        """å‘å¸ƒæ–°é—»æ•°æ®åˆ°NATS JetStream"""
        if self.nats_client is None or self.js is None:
            await self.connect_nats()

        if self.nats_client is None or self.js is None:
            print("âŒ NATS JetStreamè¿æ¥å¤±è´¥ï¼Œè·³è¿‡å‘å¸ƒ")
            return

        try:
            # å‘é€åˆ°news.clsä¸»é¢˜
            message = json.dumps(news_data, ensure_ascii=False).encode('utf-8')

            # ä½¿ç”¨JetStreamå‘å¸ƒï¼Œä¼šè¿”å›ACK
            ack = await self.js.publish("news.cls", message)
            print(f"âœ… å·²å‘å¸ƒæ–°é—»åˆ°NATS JetStream: {news_data['title']}")
            print(f"ğŸ“Š æ¶ˆæ¯åºå·: {ack.seq}, æµ: {ack.stream}")

        except Exception as e:
            print(f"âŒ å‘å¸ƒæ–°é—»å¤±è´¥: {e}")

    def create_enhanced_crawler_config(self, headless=True):
        """åˆ›å»ºå¢å¼ºçš„çˆ¬è™«é…ç½®"""
        return {
            'headless': headless,
            'verbose': False,  # å‡å°‘æ—¥å¿—è¾“å‡º
            'browser_type': 'chromium',
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            },
            'extra_args': [
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--window-size=1920,1080',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',  # ç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥æé«˜é€Ÿåº¦
                '--disable-javascript',  # æš‚æ—¶ç¦ç”¨JSï¼Œé¿å…å¤æ‚çš„SPAæ¸²æŸ“
            ],
            'sleep_on_close': False
        }

    async def simple_crawl_with_retry(self, url, crawler, max_retries=3):
        """ç®€å•çš„çˆ¬å–æ–¹æ³•ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                print(f"ğŸŒ å°è¯•è®¿é—® ({attempt + 1}/{max_retries}): {url}")

                # ç®€åŒ–çš„é…ç½®ï¼Œé¿å…å¤æ‚çš„ç­‰å¾…æ¡ä»¶
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    page_timeout=30000,  # 30ç§’è¶…æ—¶
                    delay_before_return_html=3.0,  # ç®€åŒ–ç­‰å¾…æ—¶é—´
                    magic=False,  # ç¦ç”¨magicæ¨¡å¼
                    remove_overlay_elements=False,
                    exclude_external_links=True,
                    # ç§»é™¤wait_untilå‚æ•°ï¼Œé¿å…networkidleè¶…æ—¶
                )

                result = await crawler.arun(url=url, config=config)

                if result.success and result.html:
                    print(f"âœ… è®¿é—®æˆåŠŸ: {url}")
                    return result
                else:
                    print(f"âŒ è®¿é—®å¤±è´¥ (å°è¯• {attempt + 1}): {result.error_message if result else 'Unknown error'}")

            except Exception as e:
                print(f"âŒ è®¿é—®å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")

            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                print(f"â° ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)

        print(f"âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†: {url}")
        return None

    async def debug_specific_urls(self):
        """è°ƒè¯•ç‰¹å®šURLçš„é¡µé¢ç»“æ„"""
        test_urls = [
            "https://www.cls.cn/detail/2040068",  # ç‰¹æœ—æ™®é‡‘å¡
            "https://www.cls.cn/detail/2040052",  # ç‰¹æœ—æ™®å“ˆä½›
        ]

        # ä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®
        crawler_config = self.create_enhanced_crawler_config(headless=True)

        async with AsyncWebCrawler(**crawler_config) as crawler:
            for url in test_urls:
                print(f"\n{'='*50}")
                print(f"ğŸ” è°ƒè¯•URL: {url}")
                print(f"{'='*50}")

                # ç›´æ¥è®¿é—®ç›®æ ‡é¡µé¢ï¼Œä¸å†å…ˆè®¿é—®é¦–é¡µ
                result = await self.simple_crawl_with_retry(url, crawler)

                if result and result.success:
                    print(f"ğŸ“„ é¡µé¢HTMLé•¿åº¦: {len(result.html)}")

                    # ä¿å­˜HTMLåˆ°æ–‡ä»¶ä¾›åˆ†æ
                    debug_file = f"debug_{url.split('/')[-1]}.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(result.html)
                    print(f"ğŸ“ HTMLå·²ä¿å­˜åˆ°: {debug_file}")

                    # åˆ†æé¡µé¢å†…å®¹
                    article_data = self.extract_article_content_advanced(result.html, url)

                    print(f"\nğŸ” æ–‡ç« æå–ç»“æœ:")
                    print(f"ğŸ“° æ ‡é¢˜: {article_data.get('title', 'N/A')[:100]}...")
                    print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(article_data.get('content', ''))}")
                    print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {article_data.get('content', '')[:200]}...")
                    print(f"â° æ—¶é—´: {article_data.get('time', 'N/A')}")
                    print(f"ğŸ‘¤ ä½œè€…: {article_data.get('author', 'N/A')}")
                    print(f"âœ… æå–æˆåŠŸ: {article_data.get('success', False)}")

                    # æ£€æŸ¥é¡µé¢æ˜¯å¦è¢«é‡å®šå‘
                    if self.is_redirected_to_homepage(result.html):
                        print("\nâš ï¸  è­¦å‘Š: é¡µé¢è¢«é‡å®šå‘åˆ°é¦–é¡µ!")
                        print("å¯èƒ½çš„åŸå› :")
                        print("1. ç½‘ç«™åçˆ¬è™«æœºåˆ¶")
                        print("2. éœ€è¦ç‰¹æ®Šçš„è®¿é—®æƒé™")
                        print("3. URLæ— æ•ˆæˆ–æ–‡ç« å·²åˆ é™¤")

                        # å°è¯•å…¶ä»–æ–¹æ³•
                        print("\nğŸ”„ å°è¯•å¤‡ç”¨è®¿é—®æ–¹æ³•...")
                        await self.try_alternative_access(url, crawler)
                    else:
                        print("âœ… é¡µé¢å†…å®¹æ­£å¸¸")

                else:
                    print(f"âŒ è°ƒè¯•å¤±è´¥: {url}")

                # ç­‰å¾…ä¸€ä¸‹å†å¤„ç†ä¸‹ä¸€ä¸ªURL
                await asyncio.sleep(3)

        print("âœ… è°ƒè¯•å®Œæˆ")

    def is_redirected_to_homepage(self, html):
        """æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é¦–é¡µ"""
        indicators = [
            "è´¢è”ç¤¾-ä¸»æµè´¢ç»æ–°é—»é›†å›¢",
            "ä¸Šè¯æŒ‡æ•°",
            "æ·±è¯æˆæŒ‡",
            "åˆ›ä¸šæ¿æŒ‡",
            "çœ‹ç›˜",
            "çƒ­é—¨æ¿å—"
        ]

        # ç®€å•æ£€æŸ¥HTMLå†…å®¹
        html_lower = html.lower()

        # å¦‚æœåŒ…å«å¤ªå¤šé¦–é¡µç‰¹å¾ï¼Œè®¤ä¸ºæ˜¯é‡å®šå‘
        homepage_count = sum(1 for indicator in indicators if indicator in html)

        return homepage_count >= 3

    async def try_alternative_access(self, url, crawler):
        """å°è¯•å¤‡ç”¨è®¿é—®æ–¹æ³•"""
        alternatives = [
            # æ–¹æ³•1: æ·»åŠ éšæœºå‚æ•°
            f"{url}?t={int(datetime.datetime.now().timestamp())}",
            # æ–¹æ³•2: å°è¯•ç§»åŠ¨ç‰ˆ
            url.replace('www.cls.cn', 'm.cls.cn'),
        ]

        for i, alt_url in enumerate(alternatives):
            print(f"ğŸ”„ å°è¯•æ–¹æ³• {i+1}: {alt_url}")

            try:
                result = await self.simple_crawl_with_retry(alt_url, crawler, max_retries=2)

                if result and result.success and not self.is_redirected_to_homepage(result.html):
                    print(f"âœ… å¤‡ç”¨æ–¹æ³• {i+1} æˆåŠŸ!")
                    # ä¿å­˜æˆåŠŸçš„HTML
                    success_file = f"success_{url.split('/')[-1]}_method{i+1}.html"
                    with open(success_file, 'w', encoding='utf-8') as f:
                        f.write(result.html)
                    print(f"ğŸ“ æˆåŠŸçš„HTMLå·²ä¿å­˜åˆ°: {success_file}")
                    return result
                else:
                    print(f"âŒ å¤‡ç”¨æ–¹æ³• {i+1} å¤±è´¥")

            except Exception as e:
                print(f"âŒ å¤‡ç”¨æ–¹æ³• {i+1} å‡ºé”™: {e}")

            await asyncio.sleep(2)

        return None

    def extract_article_content_advanced(self, html_content, url):
        """é«˜çº§å†…å®¹æå–æ–¹æ³•"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # æ£€æŸ¥æ˜¯å¦æ˜¯é¦–é¡µ
        if self.is_redirected_to_homepage(html_content):
            print("âš ï¸  æ£€æµ‹åˆ°é¦–é¡µé‡å®šå‘ï¼Œå°è¯•ä»HTMLä¸­æå–å¯èƒ½çš„ä¿¡æ¯")
            return self.extract_from_homepage_html(html_content, url)

        # æ–¹æ³•1: ä»JSONæ•°æ®æå–
        title, content, time_str, author = self.extract_from_script_json(html_content)

        # æ–¹æ³•2: ä»DOMå…ƒç´ æå–
        if not content or len(content) < 50:
            title2, content2, time_str2, author2 = self.extract_from_dom_elements(soup)
            if len(content2) > len(content):
                title, content, time_str, author = title2, content2, time_str2, author2

        # æ–¹æ³•3: æ™ºèƒ½æ–‡æœ¬æå–
        if not content or len(content) < 30:
            title3, content3, time_str3, author3 = self.extract_from_smart_text(html_content)
            if len(content3) > len(content):
                title, content, time_str, author = title3, content3, time_str3, author3

        return {
            'title': title or f"è´¢è”ç¤¾æ–°é—»_{url.split('/')[-1]}",
            'content': content or "",
            'time': time_str or datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'author': author or "è´¢è”ç¤¾",
            'success': bool(title and content and len(content) > 30)
        }

    def extract_from_homepage_html(self, html_content, url):
        """ä»é¦–é¡µHTMLä¸­å°è¯•æå–ç›¸å…³æ–°é—»ä¿¡æ¯"""
        # ä»URLæå–æ–‡ç« ID
        article_id = url.split('/')[-1] if '/' in url else ""

        # åœ¨é¦–é¡µHTMLä¸­æœç´¢ç›¸å…³çš„æ–°é—»ç‰‡æ®µ
        soup = BeautifulSoup(html_content, 'html.parser')

        # æŸ¥æ‰¾å¯èƒ½åŒ…å«ç›®æ ‡æ–‡ç« ä¿¡æ¯çš„å…ƒç´ 
        links = soup.find_all('a', href=re.compile(f'/detail/{article_id}'))

        title = ""
        content = ""

        for link in links:
            # è·å–é“¾æ¥æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
            link_text = link.get_text().strip()
            if link_text and len(link_text) > 8:
                title = link_text

                # æŸ¥æ‰¾ç›¸é‚»çš„å†…å®¹å…ƒç´ 
                parent = link.parent
                if parent:
                    siblings = parent.find_next_siblings()
                    for sibling in siblings[:3]:  # æ£€æŸ¥åé¢å‡ ä¸ªå…„å¼Ÿå…ƒç´ 
                        sibling_text = sibling.get_text().strip()
                        if len(sibling_text) > 50:
                            content = sibling_text
                            break
                break

        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ–‡ç« IDä½œä¸ºæ ‡é¢˜
        if not title:
            title = f"è´¢è”ç¤¾æ–°é—»_{article_id}"

        return {
            'title': title,
            'content': content,
            'time': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'author': "è´¢è”ç¤¾",
            'success': bool(title)
        }

    def extract_from_script_json(self, html_content):
        """ä»é¡µé¢JSONæ•°æ®ä¸­æå–"""
        # æŸ¥æ‰¾ __NEXT_DATA__
        next_data_pattern = r'<script id="__NEXT_DATA__" type="application/json">(.*?)</script>'
        match = re.search(next_data_pattern, html_content, re.DOTALL)

        if match:
            try:
                next_data = json.loads(match.group(1))

                # é€’å½’æœç´¢æ–‡ç« æ•°æ®
                article_data = self.find_article_data_in_json(next_data)

                if article_data:
                    title = article_data.get('title', '')
                    content = article_data.get('content', '') or article_data.get('brief', '')

                    # æ¸…ç†HTMLæ ‡ç­¾
                    if content:
                        content = self.clean_html_content(content)

                    # å¤„ç†æ—¶é—´
                    ctime = article_data.get('ctime', 0)
                    time_str = self.convert_timestamp(ctime)

                    # å¤„ç†ä½œè€…
                    author_info = article_data.get('author', {})
                    if isinstance(author_info, dict):
                        author = author_info.get('name', 'è´¢è”ç¤¾')
                    else:
                        author = str(author_info) if author_info else 'è´¢è”ç¤¾'

                    print(f"ğŸ“‹ JSONæå–æˆåŠŸ: æ ‡é¢˜={len(title)}å­—, å†…å®¹={len(content)}å­—")
                    return title, content, time_str, author

            except Exception as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")

        return "", "", "", ""

    def find_article_data_in_json(self, data, depth=0):
        """é€’å½’æŸ¥æ‰¾JSONä¸­çš„æ–‡ç« æ•°æ®"""
        if depth > 10:  # é˜²æ­¢æ— é™é€’å½’
            return None

        if isinstance(data, dict):
            # æŸ¥æ‰¾æ–‡ç« ç›¸å…³çš„key
            article_keys = ['articleDetail', 'detail', 'article', 'news', 'content']
            for key in article_keys:
                if key in data:
                    article_data = data[key]
                    if isinstance(article_data, dict) and article_data.get('title'):
                        return article_data

            # é€’å½’æœç´¢
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self.find_article_data_in_json(value, depth + 1)
                    if result:
                        return result

        elif isinstance(data, list):
            for item in data:
                if isinstance(item, (dict, list)):
                    result = self.find_article_data_in_json(item, depth + 1)
                    if result:
                        return result

        return None

    def extract_from_dom_elements(self, soup):
        """ä»DOMå…ƒç´ ä¸­æå–"""
        title = ""
        content = ""
        time_str = ""
        author = ""

        # æå–æ ‡é¢˜
        title_selectors = [
            'h1',
            '.detail-title',
            '.article-title',
            '[class*="title"]'
        ]

        for selector in title_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if 8 < len(text) < 200 and not any(skip in text for skip in ['è´¢è”ç¤¾-ä¸»æµ', 'å…³äºæˆ‘ä»¬']):
                    title = text
                    break
            if title:
                break

        # æå–å†…å®¹
        content_selectors = [
            '.detail-content',
            '.article-content',
            '.content-main',
            '[class*="content"]'
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            for elem in elements:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in elem.find_all(['script', 'style']):
                    script.decompose()
                text = elem.get_text().strip()
                if len(text) > 100:
                    content = text
                    break
            if content:
                break

        # æå–æ—¶é—´
        time_elements = soup.find_all(class_=re.compile(r'time|date'))
        for elem in time_elements:
            text = elem.get_text().strip()
            if re.search(r'\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}', text):
                time_str = text
                break

        # æå–ä½œè€…
        author_elements = soup.find_all(class_=re.compile(r'author|reporter|editor'))
        for elem in author_elements:
            text = elem.get_text().strip()
            if text and len(text) < 20:
                author = text
                break

        print(f"ğŸ“‹ DOMæå–: æ ‡é¢˜={len(title)}å­—, å†…å®¹={len(content)}å­—")
        return title, content, time_str, author

    def extract_from_smart_text(self, html_content):
        """æ™ºèƒ½æ–‡æœ¬æå–"""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '\n', html_content)
        lines = [line.strip() for line in text.split('\n') if line.strip()]

        title = ""
        content = ""
        time_str = ""
        author = ""

        # æŸ¥æ‰¾æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨å‰é¢ä¸”é•¿åº¦é€‚ä¸­ï¼‰
        for i, line in enumerate(lines[:50]):
            if (8 <= len(line) <= 150 and
                    not line.startswith(('å…³äºæˆ‘ä»¬', 'ç½‘ç«™å£°æ˜', 'è”ç³»æ–¹å¼', 'http')) and
                    not line.endswith(('.com', '.cn')) and
                    'Â©' not in line and
                    any(char in line for char in ['ã€', 'ï¼š', 'ï¼Œ', 'ï¼', 'ï¼Ÿ', '"', '"'])):
                title = line
                break

        # æŸ¥æ‰¾å†…å®¹ï¼ˆå¯»æ‰¾è¾ƒé•¿çš„æ®µè½ï¼‰
        content_lines = []
        in_content = False

        for line in lines:
            # è·³è¿‡å¯¼èˆªç­‰
            if any(skip in line for skip in ['å…³äºæˆ‘ä»¬', 'ç½‘ç«™å£°æ˜', 'è”ç³»æ–¹å¼', 'å¸®åŠ©', 'é¦–é¡µ']):
                continue

            # å¼€å§‹æ”¶é›†å†…å®¹çš„æ ‡å¿—
            if ('è´¢è”ç¤¾' in line and ('è®¯' in line or 'ç”µ' in line)) or \
                    (len(line) > 20 and ('ã€‚' in line or 'ï¼Œ' in line)):
                in_content = True

            if in_content and len(line) > 15:
                content_lines.append(line)

            if len(content_lines) > 20:
                break

        content = '\n'.join(content_lines[:15])

        # æŸ¥æ‰¾æ—¶é—´
        for line in lines[:100]:
            time_match = re.search(r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥\s]*\d{1,2}:\d{2})', line)
            if time_match:
                time_str = time_match.group(1)
                break

        # æŸ¥æ‰¾ä½œè€…
        for line in lines[:100]:
            author_match = re.search(r'è´¢è”ç¤¾[è®°è€…ç¼–è¾‘]*\s*([^\s\n]+)', line)
            if author_match:
                author = author_match.group(1)
                break

        print(f"ğŸ“‹ æ™ºèƒ½æå–: æ ‡é¢˜={len(title)}å­—, å†…å®¹={len(content)}å­—")
        return title, content, time_str, author

    def clean_html_content(self, content):
        """æ¸…ç†HTMLå†…å®¹"""
        if not content:
            return ""

        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        # è§£ç HTMLå®ä½“
        content = html.unescape(content)
        # æ¸…ç†ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content).strip()

        return content

    def convert_timestamp(self, timestamp):
        """è½¬æ¢æ—¶é—´æˆ³"""
        if not timestamp or not isinstance(timestamp, (int, float)):
            return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            if timestamp > 1e10:  # æ¯«ç§’çº§æ—¶é—´æˆ³
                timestamp = timestamp / 1000
            return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
        except (ValueError, OSError):
            return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    async def crawl_url_list(self, crawler=None):
        # æ›´æ–°CSSé€‰æ‹©å™¨ï¼Œæ ¹æ®è´¢è”ç¤¾çš„å®é™…é¡µé¢ç»“æ„
        schema = {
            'name': 'caijingwang toutiao page crawler',
            'baseSelector': 'body',
            'fields': [
                {
                    'name': 'all_links',
                    'selector': 'a[href*="/detail/"]',
                    'type': 'nested_list',
                    'fields': [
                        {'name': 'href', 'type': 'attribute', 'attribute':'href'},
                        {'name': 'text', 'type': 'text'}
                    ]
                }
            ]
        }

        results = {}

        menu_dict = {
            '1000': 'å¤´æ¡',
            '1003': 'Aè‚¡',
            '1007': 'ç¯çƒ'
        }

        if crawler is None:
            print("è­¦å‘Šï¼šæ²¡æœ‰ä¼ å…¥crawlerå®ä¾‹")
            return {}

        for k, v in menu_dict.items():
            url = f'https://www.cls.cn/depth?id={k}'
            print(f"æ­£åœ¨æŠ“å– {v} é¡µé¢: {url}")
            links = []

            # é‡è¯•é€»è¾‘
            max_retries = 3
            for retry in range(max_retries):
                try:
                    config = CrawlerRunConfig(
                        extraction_strategy=JsonCssExtractionStrategy(schema, verbose=False),
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=30000,  # å‡å°‘è¶…æ—¶æ—¶é—´
                        delay_before_return_html=5.0,
                        magic=False,  # ç¦ç”¨magicæ¨¡å¼
                        remove_overlay_elements=False,
                        process_iframes=False,
                        exclude_external_links=True,
                        js_only=False,
                        screenshot=False,
                        # ç§»é™¤wait_untilå‚æ•°
                    )

                    result = await crawler.arun(url=url, config=config)

                    if not result.success:
                        print(f"æŠ“å–å¤±è´¥: {url}")
                        continue

                    extracted_data = json.loads(result.extracted_content)
                    print(f"æå–çš„æ•°æ®ç»“æ„: {list(extracted_data[0].keys()) if extracted_data else 'empty'}")

                    # å¤„ç†æå–çš„é“¾æ¥
                    if extracted_data and 'all_links' in extracted_data[0]:
                        all_links = extracted_data[0]['all_links']
                        news_links = []
                        for link_obj in all_links:
                            if 'href' in link_obj:
                                href = link_obj['href']
                                if '/detail/' in href and not href.startswith('http'):
                                    news_links.append(href)

                        links = list(set(news_links))  # å»é‡
                        print(f"æ‰¾åˆ° {len(links)} ä¸ªæ–°é—»é“¾æ¥")

                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

                except Exception as e:
                    print(f'error {url} (å°è¯• {retry+1}/{max_retries}): {str(e)}')
                    if retry == max_retries - 1:
                        print(f'æ”¾å¼ƒæŠ“å– {url}')
                    else:
                        await asyncio.sleep(3)

            if links:
                for link in links:
                    full_url = f'{self.url}{link}' if not link.startswith('http') else link
                    results[full_url] = v

        print(f"æ€»å…±æ‰¾åˆ° {len(results)} ä¸ªæ–°é—»é“¾æ¥")
        return results

    async def crawl_newsletter(self, url, category, crawler):
        """çˆ¬å–å•ç¯‡æ–°é—»"""
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–°é—»: {url}")

        result = await self.simple_crawl_with_retry(url, crawler)

        if not result or not result.success:
            print(f"âŒ çˆ¬å–å¤±è´¥: {url}")
            return {}

        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°é¦–é¡µ
        if self.is_redirected_to_homepage(result.html):
            print(f"âš ï¸  é¡µé¢è¢«é‡å®šå‘ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•: {url}")
            result = await self.try_alternative_access(url, crawler)
            if not result:
                print(f"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥: {url}")
                return {}

        # æå–æ–‡ç« å†…å®¹
        article_data = self.extract_article_content_advanced(result.html, url)

        if not article_data['success']:
            print(f"âš ï¸  å†…å®¹æå–å¤±è´¥: {url}")
            return {}

        # æ„å»ºè¿”å›æ•°æ®
        news_data = {
            'title': article_data['title'],
            'abstract': article_data['content'][:200] + '...' if len(article_data['content']) > 200 else article_data['content'],
            'date': article_data['time'],
            'link': url,
            'content': article_data['content'],
            'id': md5(url),
            'type': category,
            'author': article_data['author'],
            'read_number': 0,
            'time': datetime.datetime.now().strftime('%Y-%m-%d')
        }

        print(f"âœ… æå–æˆåŠŸ: {article_data['title'][:50]}...")
        return news_data

    async def crawl(self, debug_mode=False):
        """ä¸»çˆ¬å–æ–¹æ³•"""
        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œåªè¿è¡Œè°ƒè¯•åŠŸèƒ½
        if debug_mode:
            print("ğŸ” è¿›å…¥è°ƒè¯•æ¨¡å¼...")
            await self.debug_specific_urls()
            return []

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs('data', exist_ok=True)

        # è¿æ¥NATS JetStream
        try:
            await self.connect_nats()
        except Exception as e:
            print(f"NATS JetStreamè¿æ¥å¤±è´¥ï¼Œå°†è·³è¿‡æ¶ˆæ¯å‘å¸ƒ: {e}")

        # åˆ›å»ºæµè§ˆå™¨å®ä¾‹
        crawler_config = self.create_enhanced_crawler_config()

        async with AsyncWebCrawler(**crawler_config) as crawler:
            print("å¼€å§‹è·å–æ–°é—»é“¾æ¥...")
            link_2_category = await self.crawl_url_list(crawler=crawler)

            if not link_2_category:
                print("æœªè·å–åˆ°ä»»ä½•æ–°é—»é“¾æ¥ï¼Œé€€å‡º...")
                return []

            print(f"è·å–åˆ° {len(link_2_category)} ä¸ªæ–°é—»é“¾æ¥ï¼Œå¼€å§‹æŠ“å–è¯¦æƒ…...")

            processed_count = 0
            for link, category in link_2_category.items():
                _id = md5(link)
                if _id in self.history:
                    print(f"è·³è¿‡å·²å­˜åœ¨çš„æ–°é—»: {link}")
                    continue

                print(f"æ­£åœ¨å¤„ç†ç¬¬ {processed_count + 1} ä¸ªæ–°é—»: {link}")

                try:
                    news = await self.crawl_newsletter(link, category, crawler)

                    if not news or not news.get('title') or len(news.get('title', '').strip()) < 5:
                        print(f"æ–°é—»å†…å®¹æå–å¤±è´¥ï¼Œè·³è¿‡: {link}")
                        continue

                    print(f"ä¿å­˜æ–°é—»: {news['title']}")
                    self.save(news)

                    # å‘å¸ƒåˆ°NATS JetStream
                    await self.publish_news(news)

                    processed_count += 1

                    # æ¯å¤„ç†3ä¸ªæ–°é—»ä¼‘æ¯ä¸€ä¸‹
                    if processed_count % 3 == 0:
                        print(f"å·²å¤„ç† {processed_count} ä¸ªæ–°é—»ï¼Œæš‚åœ3ç§’...")
                        await asyncio.sleep(3)

                except Exception as e:
                    print(f'å¤„ç†æ–°é—»è¯¦æƒ…å¤±è´¥: {link} - {str(e)}')
                    continue

        # å…³é—­NATSè¿æ¥
        if self.nats_client:
            try:
                await self.nats_client.close()
                print("ğŸ”š NATS JetStreamè¿æ¥å·²å…³é—­")
            except Exception as e:
                print(f"âŒ å…³é—­NATS JetStreamè¿æ¥å¤±è´¥: {e}")

        print(f"çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–°é—»")
        return await self.get_last_day_data()


async def test_nats_jetstream_connection():
    """æµ‹è¯•NATS JetStreamè¿æ¥"""
    try:
        nc = NATS()
        await nc.connect("nats://localhost:4222")
        print("âœ… NATSè¿æ¥æµ‹è¯•æˆåŠŸ")

        js = nc.jetstream()

        # å°è¯•å‘å¸ƒä¸€æ¡æµ‹è¯•æ¶ˆæ¯
        test_msg = {"test": "message", "timestamp": datetime.datetime.now().isoformat()}
        ack = await js.publish("test.subject", json.dumps(test_msg).encode())
        print(f"âœ… JetStreamå‘å¸ƒæµ‹è¯•æˆåŠŸï¼Œåºå·: {ack.seq}")

        await nc.close()
        return True
    except Exception as e:
        print(f"âŒ NATS JetStreamè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == '__main__':
    import sys

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    debug_mode = '--debug' in sys.argv

    if debug_mode:
        print("ğŸ” å¯åŠ¨è°ƒè¯•æ¨¡å¼...")
        crawler = CLSCrawler()
        asyncio.run(crawler.crawl(debug_mode=True))
    else:
        # å…ˆæµ‹è¯•NATS JetStreamè¿æ¥
        asyncio.run(test_nats_jetstream_connection())

        # ç„¶åè¿è¡Œçˆ¬è™«
        asyncio.run(CLSCrawler().crawl())