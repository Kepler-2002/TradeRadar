import asyncio
import datetime
import hashlib
import json
import os
import re
from typing import Dict, Any, Union, List
import html

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
# ä½¿ç”¨æ ‡å‡†çš„ NATS åº“ï¼Œæ”¯æŒ JetStream
from nats.aio.client import Client as NATS


def md5(text):
    m = hashlib.md5()
    m.update(text.encode('utf-8'))
    return m.hexdigest()


def get_datas(file_path, json_flag=True, all_flag=False, mode='r'):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
    results = []

    if not os.path.exists(file_path):
        return results

    try:
        with open(file_path, mode, encoding='utf-8') as f:
            for line in f.readlines():
                if json_flag:
                    try:
                        results.append(json.loads(line.strip()))
                    except json.JSONDecodeError:
                        continue
                else:
                    results.append(line.strip())
        if all_flag:
            if json_flag:
                return json.loads(''.join(results))
            else:
                return '\n'.join(results)
        return results
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return results


def save_datas(file_path, datas, json_flag=True, all_flag=False, with_indent=False, mode='w'):
    """ä¿å­˜æ–‡æœ¬æ–‡ä»¶"""
    try:
        with open(file_path, mode, encoding='utf-8') as f:
            if all_flag:
                if json_flag:
                    f.write(json.dumps(datas, ensure_ascii=False, indent= 4 if with_indent else None))
                else:
                    f.write(''.join(datas))
            else:
                for data in datas:
                    if json_flag:
                        f.write(json.dumps(data, ensure_ascii=False) + '\n')
                    else:
                        f.write(data + '\n')
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")


class AbstractAICrawler():

    def __init__(self) -> None:
        pass
    def crawl(self):
        raise NotImplementedError()


class AINewsCrawler(AbstractAICrawler):
    def __init__(self, domain) -> None:
        super().__init__()
        self.domain = domain
        self.file_path = f'data/{self.domain}.json'
        self.history = self.init()

    def init(self):
        if not os.path.exists(self.file_path):
            return {}
        try:
            data = get_datas(self.file_path)
            return {ele['id']: ele for ele in data if 'id' in ele}
        except Exception as e:
            print(f"åˆå§‹åŒ–å†å²æ•°æ®å¤±è´¥: {e}")
            return {}

    def save(self, datas: Union[List, Dict]):
        if isinstance(datas, dict):
            datas = [datas]
        self.history.update({ele['id']: ele for ele in datas})
        save_datas(self.file_path, datas=list(self.history.values()))


class FinanceNewsCrawler(AINewsCrawler):

    def __init__(self, domain='') -> None:
        super().__init__(domain)

    def save(self, datas: Union[List, Dict]):
        if isinstance(datas, dict):
            datas = [datas]
        self.history.update({ele['id']: ele for ele in datas})

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯æ–°å»º
        file_exists = os.path.exists(self.file_path)
        mode = 'a' if file_exists else 'w'

        save_datas(self.file_path, datas=datas, mode=mode)
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {self.file_path}ï¼Œæ¨¡å¼: {mode}")

    async def get_last_day_data(self):
        last_day = (datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
        datas = self.init()
        return [v for v in datas.values() if last_day in v.get('date', '')]


class CLSCrawler(FinanceNewsCrawler):
    """è´¢è”ç¤¾æ–°é—»æŠ“å– - æ”¯æŒSPAåº”ç”¨"""

    def __init__(self) -> None:
        self.domain = 'cls'
        super().__init__(self.domain)
        self.url = 'https://www.cls.cn'
        self.nats_client = None
        self.js = None  # JetStream ä¸Šä¸‹æ–‡

    async def connect_nats(self):
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨å¹¶è®¾ç½®JetStream"""
        try:
            self.nats_client = NATS()
            await self.nats_client.connect("nats://localhost:4222")
            print("âœ… å·²è¿æ¥åˆ°NATSæœåŠ¡å™¨")

            # è·å–JetStreamä¸Šä¸‹æ–‡
            self.js = self.nats_client.jetstream()

            # åˆ›å»ºStreamï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            try:
                await self.js.add_stream(
                    name="NEWS_STREAM",
                    subjects=["news.>"],  # åŒ¹é…æ‰€æœ‰ä»¥news.å¼€å¤´çš„ä¸»é¢˜
                    retention="limits",   # åŸºäºé™åˆ¶çš„ä¿ç•™ç­–ç•¥
                    max_msgs=10000,      # æœ€å¤§æ¶ˆæ¯æ•°
                    max_bytes=100*1024*1024,  # æœ€å¤§å­—èŠ‚æ•° 100MB
                    max_age=7*24*3600    # ä¿ç•™7å¤©
                )
                print("âœ… åˆ›å»ºäº†JetStreamæµ: NEWS_STREAM")
            except Exception as e:
                if "stream name already in use" in str(e):
                    print("ğŸ“‹ JetStreamæµ NEWS_STREAM å·²å­˜åœ¨")
                else:
                    print(f"âš ï¸ åˆ›å»ºJetStreamæµæ—¶å‡ºé”™: {e}")

        except Exception as e:
            print(f"âŒ è¿æ¥NATSå¤±è´¥: {e}")
            self.nats_client = None
            self.js = None

    async def publish_news(self, news_data):
        """å‘å¸ƒæ–°é—»æ•°æ®åˆ°NATS JetStream"""
        if self.nats_client is None or self.js is None:
            await self.connect_nats()

        if self.nats_client is None or self.js is None:
            print("âŒ NATS JetStreamè¿æ¥å¤±è´¥ï¼Œè·³è¿‡å‘å¸ƒ")
            return

        try:
            # å‘é€åˆ°news.clsä¸»é¢˜
            message = json.dumps(news_data, ensure_ascii=False).encode('utf-8')

            # ä½¿ç”¨JetStreamå‘å¸ƒï¼Œä¼šè¿”å›ACK
            ack = await self.js.publish("news.cls", message)
            print(f"âœ… å·²å‘å¸ƒæ–°é—»åˆ°NATS JetStream: {news_data['title']}")
            print(f"ğŸ“Š æ¶ˆæ¯åºå·: {ack.seq}, æµ: {ack.stream}")

        except Exception as e:
            print(f"âŒ å‘å¸ƒæ–°é—»å¤±è´¥: {e}")

    async def debug_rendered_page(self, url, crawler):
        """è°ƒè¯•å·²æ¸²æŸ“é¡µé¢çš„å®é™…DOMç»“æ„"""

        js_commands = [
            """
            (async () => {
                console.log('ğŸ” å¼€å§‹è°ƒè¯•é¡µé¢:', window.location.href);
                
                // ç­‰å¾…é¡µé¢å®Œå…¨æ¸²æŸ“
                await new Promise(resolve => setTimeout(resolve, 10000));
                
                // ç­‰å¾…å†…å®¹åŠ è½½
                let attempts = 0;
                while (attempts < 20) {
                    const titleElements = document.querySelectorAll('h1, h2, [class*="title"]');
                    const contentElements = document.querySelectorAll('[class*="content"], [class*="article"], [class*="detail"]');
                    
                    if (titleElements.length > 0 && contentElements.length > 0) {
                        console.log('âœ… å†…å®¹å·²åŠ è½½');
                        break;
                    }
                    
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    attempts++;
                }
                
                // åˆ†æé¡µé¢ç»“æ„
                console.log('=== ğŸ” é¡µé¢ç»“æ„åˆ†æ ===');
                
                // æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ ‡é¢˜å…ƒç´ 
                const titleSelectors = ['h1', 'h2', 'h3', '[class*="title"]', '[class*="headline"]'];
                titleSelectors.forEach(selector => {
                    const elements = document.querySelectorAll(selector);
                    elements.forEach((el, index) => {
                        if (el.textContent.trim().length > 10) {
                            console.log(`ğŸ“° æ ‡é¢˜å€™é€‰ ${selector}[${index}]:`, el.textContent.trim().substring(0, 50));
                            console.log(`  - ç±»å:`, el.className);
                            console.log(`  - é•¿åº¦:`, el.textContent.trim().length);
                        }
                    });
                });
                
                // æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å†…å®¹å…ƒç´ 
                const contentSelectors = ['[class*="content"]', '[class*="article"]', '[class*="detail"]', '[class*="text"]'];
                contentSelectors.forEach(selector => {
                    const elements = document.querySelectorAll(selector);
                    elements.forEach((el, index) => {
                        if (el.textContent.trim().length > 100) {
                            console.log(`ğŸ“ å†…å®¹å€™é€‰ ${selector}[${index}]:`, el.textContent.trim().substring(0, 100));
                            console.log(`  - ç±»å:`, el.className);
                            console.log(`  - é•¿åº¦:`, el.textContent.trim().length);
                        }
                    });
                });
                
                // æŸ¥æ‰¾æ—¶é—´å…ƒç´ 
                const timeSelectors = ['[class*="time"]', '[class*="date"]', 'time'];
                timeSelectors.forEach(selector => {
                    const elements = document.querySelectorAll(selector);
                    elements.forEach((el, index) => {
                        console.log(`â° æ—¶é—´å€™é€‰ ${selector}[${index}]:`, el.textContent.trim());
                        console.log(`  - ç±»å:`, el.className);
                    });
                });
                
                // æŸ¥æ‰¾ä½œè€…å…ƒç´ 
                const authorSelectors = ['[class*="author"]', '[class*="reporter"]', '[class*="editor"]'];
                authorSelectors.forEach(selector => {
                    const elements = document.querySelectorAll(selector);
                    elements.forEach((el, index) => {
                        console.log(`ğŸ‘¤ ä½œè€…å€™é€‰ ${selector}[${index}]:`, el.textContent.trim());
                        console.log(`  - ç±»å:`, el.className);
                    });
                });
                
                return true;
            })();
            """
        ]

        try:
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=120000,
                delay_before_return_html=15.0,
                js_code=js_commands,
                magic=True,
                remove_overlay_elements=False,
                process_iframes=False,
                exclude_external_links=True,
                wait_until='networkidle'
            )

            result = await crawler.arun(url=url, config=config)

            if result.success:
                print(f"ğŸ” è°ƒè¯•é¡µé¢: {url}")
                print(f"ğŸ“„ é¡µé¢HTMLé•¿åº¦: {len(result.html)}")

                # ä¿å­˜HTMLåˆ°æ–‡ä»¶ä¾›åˆ†æ
                debug_file = f"debug_{url.split('/')[-1]}.html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(result.html)
                print(f"ğŸ“ HTMLå·²ä¿å­˜åˆ°: {debug_file}")

                # ç”¨BeautifulSoupåˆ†æç»“æ„
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(result.html, 'html.parser')

                print("\nğŸ” DOMç»“æ„åˆ†æ:")

                # æŸ¥æ‰¾æ ‡é¢˜
                print("\nğŸ“° æ ‡é¢˜å…ƒç´ :")
                title_elements = soup.find_all(['h1', 'h2', 'h3']) + soup.find_all(class_=re.compile(r'title|headline'))
                for i, elem in enumerate(title_elements[:5]):
                    text = elem.get_text().strip()
                    if len(text) > 10:
                        print(f"  {i+1}. æ ‡ç­¾: {elem.name}, ç±»: {elem.get('class', [])}")
                        print(f"     æ–‡æœ¬: {text[:80]}...")

                # æŸ¥æ‰¾å†…å®¹
                print("\nğŸ“ å†…å®¹å…ƒç´ :")
                content_elements = soup.find_all(class_=re.compile(r'content|article|detail|text'))
                for i, elem in enumerate(content_elements[:5]):
                    text = elem.get_text().strip()
                    if len(text) > 100:
                        print(f"  {i+1}. æ ‡ç­¾: {elem.name}, ç±»: {elem.get('class', [])}")
                        print(f"     æ–‡æœ¬é•¿åº¦: {len(text)}")
                        print(f"     æ–‡æœ¬é¢„è§ˆ: {text[:100]}...")

                # æŸ¥æ‰¾æ—¶é—´
                print("\nâ° æ—¶é—´å…ƒç´ :")
                time_elements = soup.find_all(class_=re.compile(r'time|date')) + soup.find_all('time')
                for i, elem in enumerate(time_elements[:3]):
                    text = elem.get_text().strip()
                    print(f"  {i+1}. æ ‡ç­¾: {elem.name}, ç±»: {elem.get('class', [])}")
                    print(f"     æ–‡æœ¬: {text}")

                # åŸºäºè°ƒè¯•ç»“æœï¼Œå°è¯•æå–çœŸå®çš„CSSé€‰æ‹©å™¨
                print("\nğŸ¯ æ¨èçš„CSSé€‰æ‹©å™¨:")

                # åˆ†ææœ€å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
                best_title_selector = self.analyze_best_title_selector(soup)
                if best_title_selector:
                    print(f"ğŸ“° æ ‡é¢˜é€‰æ‹©å™¨: {best_title_selector}")

                # åˆ†ææœ€å¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
                best_content_selector = self.analyze_best_content_selector(soup)
                if best_content_selector:
                    print(f"ğŸ“ å†…å®¹é€‰æ‹©å™¨: {best_content_selector}")

            return result.html if result.success else ""

        except Exception as e:
            print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
            return ""

    def analyze_best_title_selector(self, soup):
        """åˆ†ææœ€ä½³çš„æ ‡é¢˜é€‰æ‹©å™¨"""
        candidates = []

        # æ£€æŸ¥h1æ ‡ç­¾
        h1_elements = soup.find_all('h1')
        for elem in h1_elements:
            text = elem.get_text().strip()
            if 10 < len(text) < 200 and not any(skip in text for skip in ['è´¢è”ç¤¾-ä¸»æµ', 'å…³äºæˆ‘ä»¬', 'ç½‘ç«™å£°æ˜']):
                candidates.append(('h1', elem.get('class', []), len(text), text[:50]))

        # æ£€æŸ¥å¸¦titleç±»çš„å…ƒç´ 
        title_elements = soup.find_all(class_=re.compile(r'title'))
        for elem in title_elements:
            text = elem.get_text().strip()
            if 10 < len(text) < 200 and not any(skip in text for skip in ['è´¢è”ç¤¾-ä¸»æµ', 'å…³äºæˆ‘ä»¬', 'ç½‘ç«™å£°æ˜']):
                classes = elem.get('class', [])
                selector = f".{' '.join(classes)}" if classes else elem.name
                candidates.append((selector, classes, len(text), text[:50]))

        if candidates:
            # é€‰æ‹©æœ€åˆé€‚çš„ï¼ˆæ–‡æœ¬é•¿åº¦é€‚ä¸­çš„ï¼‰
            best = min(candidates, key=lambda x: abs(x[2] - 30))  # åå¥½30å­—ç¬¦å·¦å³çš„æ ‡é¢˜
            return best[0]

        return None

    def analyze_best_content_selector(self, soup):
        """åˆ†ææœ€ä½³çš„å†…å®¹é€‰æ‹©å™¨"""
        candidates = []

        # æ£€æŸ¥å¸¦content/article/detailç±»çš„å…ƒç´ 
        content_patterns = ['content', 'article', 'detail']
        for pattern in content_patterns:
            elements = soup.find_all(class_=re.compile(pattern))
            for elem in elements:
                text = elem.get_text().strip()
                if len(text) > 200:  # å†…å®¹åº”è¯¥æ¯”è¾ƒé•¿
                    classes = elem.get('class', [])
                    selector = f".{' '.join(classes)}" if classes else elem.name
                    candidates.append((selector, classes, len(text), text[:100]))

        if candidates:
            # é€‰æ‹©æ–‡æœ¬æœ€é•¿çš„ä½œä¸ºä¸»è¦å†…å®¹
            best = max(candidates, key=lambda x: x[2])
            return best[0]

        return None

    async def debug_specific_urls(self):
        """è°ƒè¯•ç‰¹å®šURLçš„é¡µé¢ç»“æ„"""
        test_urls = [
            "https://www.cls.cn/detail/2040068",  # ç‰¹æœ—æ™®é‡‘å¡
            "https://www.cls.cn/detail/2040052",  # ç‰¹æœ—æ™®å“ˆä½›
        ]

        async with AsyncWebCrawler(verbose=True, headless=False) as crawler:  # è®¾ç½®headless=Falseå¯ä»¥çœ‹åˆ°æµè§ˆå™¨
            for url in test_urls:
                print(f"\n{'='*50}")
                print(f"ğŸ” è°ƒè¯•URL: {url}")
                print(f"{'='*50}")

                html = await self.debug_rendered_page(url, crawler)
                if html:
                    print("âœ… è°ƒè¯•å®Œæˆ")
                else:
                    print("âŒ è°ƒè¯•å¤±è´¥")

                # ç­‰å¾…ä¸€ä¸‹å†å¤„ç†ä¸‹ä¸€ä¸ªURL
                await asyncio.sleep(5)

    async def crawl_url_list(self, crawler=None):
        # æ›´æ–°CSSé€‰æ‹©å™¨ï¼Œæ ¹æ®è´¢è”ç¤¾çš„å®é™…é¡µé¢ç»“æ„
        schema = {
            'name': 'caijingwang toutiao page crawler',
            'baseSelector': 'body',
            'fields': [
                {
                    'name': 'all_links',
                    'selector': 'a[href*="/detail/"]',
                    'type': 'nested_list',
                    'fields': [
                        {'name': 'href', 'type': 'attribute', 'attribute':'href'},
                        {'name': 'text', 'type': 'text'}
                    ]
                }
            ]
        }

        # å¢å¼ºçš„JSä»£ç ï¼Œç­‰å¾…SPAå®Œå…¨æ¸²æŸ“
        js_commands = [
            """
            (async () => {
                console.log('å¼€å§‹ç­‰å¾…è´¢è”ç¤¾é¡µé¢æ¸²æŸ“...');
                await new Promise(resolve => setTimeout(resolve, 3000));
                
                // ç­‰å¾…Reactåº”ç”¨åˆå§‹åŒ–
                let retries = 0;
                while (!document.querySelector('a[href*="/detail/"]') && retries < 15) {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    retries++;
                    console.log('ç­‰å¾…é“¾æ¥æ¸²æŸ“...', retries);
                }
                
                // æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
                window.scrollTo(0, document.body.scrollHeight);
                await new Promise(resolve => setTimeout(resolve, 2000));
                
                // å°è¯•ç‚¹å‡»åŠ è½½æ›´å¤šæŒ‰é’®
                const loadMoreButton = document.querySelector('.more-button, .load-more, [class*="more"]');
                if (loadMoreButton && loadMoreButton.offsetParent !== null) {
                    loadMoreButton.click();
                    await new Promise(resolve => setTimeout(resolve, 3000));
                }
                
                console.log('è´¢è”ç¤¾é¡µé¢æ¸²æŸ“å®Œæˆ');
                return true;
            })();
            """
        ]

        results = {}

        menu_dict = {
            '1000': 'å¤´æ¡',
            '1003': 'Aè‚¡',
            '1007': 'ç¯çƒ'
        }

        if crawler is None:
            print("è­¦å‘Šï¼šæ²¡æœ‰ä¼ å…¥crawlerå®ä¾‹")
            return {}

        for k, v in menu_dict.items():
            url = f'https://www.cls.cn/depth?id={k}'
            print(f"æ­£åœ¨æŠ“å– {v} é¡µé¢: {url}")
            links = []

            # é‡è¯•é€»è¾‘
            max_retries = 3
            for retry in range(max_retries):
                try:
                    config = CrawlerRunConfig(
                        extraction_strategy=JsonCssExtractionStrategy(schema, verbose=False),
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=60000,  # å¢åŠ è¶…æ—¶æ—¶é—´
                        delay_before_return_html=8.0,  # ç­‰å¾…SPAæ¸²æŸ“
                        js_code=js_commands,
                        magic=True,
                        remove_overlay_elements=False,  # ä¸ç§»é™¤å…ƒç´ ï¼Œé¿å…å½±å“SPA
                        process_iframes=False,
                        exclude_external_links=True,
                        js_only=False,
                        screenshot=False,
                        wait_until='networkidle'
                    )

                    result = await crawler.arun(url=url, config=config)

                    if not result.success:
                        print(f"æŠ“å–å¤±è´¥: {url}")
                        continue

                    extracted_data = json.loads(result.extracted_content)
                    print(f"æå–çš„æ•°æ®ç»“æ„: {list(extracted_data[0].keys()) if extracted_data else 'empty'}")

                    # å¤„ç†æå–çš„é“¾æ¥
                    if extracted_data and 'all_links' in extracted_data[0]:
                        all_links = extracted_data[0]['all_links']
                        news_links = []
                        for link_obj in all_links:
                            if 'href' in link_obj:
                                href = link_obj['href']
                                if '/detail/' in href and not href.startswith('http'):
                                    news_links.append(href)

                        links = list(set(news_links))  # å»é‡
                        print(f"æ‰¾åˆ° {len(links)} ä¸ªæ–°é—»é“¾æ¥")

                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

                except Exception as e:
                    print(f'error {url} (å°è¯• {retry+1}/{max_retries}): {str(e)}')
                    if retry == max_retries - 1:
                        print(f'æ”¾å¼ƒæŠ“å– {url}')
                    else:
                        await asyncio.sleep(3)

            if links:
                for link in links:
                    full_url = f'{self.url}{link}' if not link.startswith('http') else link
                    results[full_url] = v

        print(f"æ€»å…±æ‰¾åˆ° {len(results)} ä¸ªæ–°é—»é“¾æ¥")
        return results

    async def crawl_newsletter(self, url, category, crawler):
        """é’ˆå¯¹è´¢è”ç¤¾SPAåº”ç”¨çš„æ–°é—»è¯¦æƒ…æŠ“å–æ–¹æ³•"""

        # å¢å¼ºçš„JavaScriptæ‰§è¡Œç­–ç•¥
        js_commands = [
            """
            (async () => {
                console.log('å¼€å§‹ç­‰å¾…æ–°é—»é¡µé¢æ¸²æŸ“...', window.location.href);
                
                // ç­‰å¾…åˆå§‹æ¸²æŸ“
                await new Promise(resolve => setTimeout(resolve, 5000));
                
                // ç­‰å¾…Next.jsæ•°æ®åŠ è½½
                let dataRetries = 0;
                while (!window.__NEXT_DATA__ && dataRetries < 15) {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    dataRetries++;
                    console.log('ç­‰å¾…Next.jsæ•°æ®...', dataRetries);
                }
                
                // ç­‰å¾…æ–‡ç« å†…å®¹æ¸²æŸ“
                let contentRetries = 0;
                while (!document.querySelector('.detail-content, .article-content, [class*="content"]') && contentRetries < 15) {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    contentRetries++;
                    console.log('ç­‰å¾…å†…å®¹æ¸²æŸ“...', contentRetries);
                }
                
                console.log('æ–°é—»é¡µé¢æ¸²æŸ“å®Œæˆ');
                
                // å°è¯•æ»šåŠ¨ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½
                window.scrollTo(0, document.body.scrollHeight);
                await new Promise(resolve => setTimeout(resolve, 2000));
                
                return true;
            })();
            """
        ]

        try:
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=90000,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°90ç§’
                delay_before_return_html=12.0,  # ç­‰å¾…12ç§’è®©SPAå®Œå…¨æ¸²æŸ“
                js_code=js_commands,
                magic=True,
                remove_overlay_elements=False,  # ä¸ç§»é™¤å…ƒç´ ï¼Œé¿å…å½±å“SPAæ¸²æŸ“
                process_iframes=False,
                exclude_external_links=True,
                wait_until='networkidle'
            )

            result = await crawler.arun(url=url, config=config)

            if not result.success:
                print(f'âŒ æŠ“å–å¤±è´¥: {url}')
                return {}

            # æ–¹æ³•1ï¼šä»Next.jsçš„JSONæ•°æ®ä¸­æå–
            title, publish_time, author, content = self.extract_from_nextjs_data(result.html, url)

            # æ–¹æ³•2ï¼šå¦‚æœJSONæå–å¤±è´¥ï¼Œå°è¯•ä»æ¸²æŸ“åçš„HTMLæå–
            if not content or len(content) < 50:
                print("ğŸ”„ JSONæå–å¤±è´¥ï¼Œå°è¯•HTMLæå–...")
                backup_title, backup_content = self.extract_from_rendered_html(result.html)
                if backup_title and not title:
                    title = backup_title
                if backup_content and len(backup_content) > len(content):
                    content = backup_content

            # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä»å…¨æ–‡æœ¬æå–
            if not content or len(content) < 30:
                print("ğŸ”„ HTMLæå–å¤±è´¥ï¼Œå°è¯•å…¨æ–‡æœ¬æå–...")
                title = self.extract_title_from_all_text(result.html, url)
                publish_time = self.extract_time_from_all_text(result.html)
                author = self.extract_author_from_all_text(result.html)
                content = self.extract_content_from_all_text(result.html)

            print(f"âœ… æå–ç»“æœ:")
            print(f"   ğŸ“° æ ‡é¢˜: {title[:80]}{'...' if len(title) > 80 else ''}")
            print(f"   â° æ—¶é—´: {publish_time}")
            print(f"   ğŸ‘¤ ä½œè€…: {author}")
            print(f"   ğŸ“ å†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")

            if len(content) < 50:
                print(f"âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½éœ€è¦è°ƒè¯•")
                print(f"ğŸ” HTMLé•¿åº¦: {len(result.html)}")

        except Exception as e:
            print(f'âŒ çˆ¬è™«é”™è¯¯: {url} - {str(e)}')
            import traceback
            traceback.print_exc()
            return {}

        return {
            'title': title if title else f"è´¢è”ç¤¾æ–°é—»_{url.split('/')[-1]}",
            'abstract': content[:200] + '...' if len(content) > 200 else content,
            'date': publish_time if publish_time else datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'link': url,
            'content': content,
            'id': md5(url),
            'type': category,
            'author': author if author else "è´¢è”ç¤¾",
            'read_number': 0,
            'time': datetime.datetime.now().strftime('%Y-%m-%d')
        }

    def extract_from_nextjs_data(self, html_content, url):
        """ä»Next.jsçš„JSONæ•°æ®ä¸­æå–æ–°é—»ä¿¡æ¯"""
        # æŸ¥æ‰¾ __NEXT_DATA__ è„šæœ¬
        next_data_pattern = r'<script id="__NEXT_DATA__" type="application/json">(.*?)</script>'
        match = re.search(next_data_pattern, html_content, re.DOTALL)

        if match:
            try:
                next_data = json.loads(match.group(1))

                # ä»JSONæ•°æ®ä¸­æå–æ–°é—»ä¿¡æ¯
                article_detail = (next_data.get('props', {})
                                  .get('initialState', {})
                                  .get('detail', {})
                                  .get('articleDetail', {}))

                title = article_detail.get('title', '')
                content = article_detail.get('content', '')
                brief = article_detail.get('brief', '')
                author_info = article_detail.get('author', {})
                author = author_info.get('name', 'è´¢è”ç¤¾') if isinstance(author_info, dict) else str(author_info)
                ctime = article_detail.get('ctime', 0)

                # æ¸…ç†HTMLæ ‡ç­¾
                if content:
                    content = self.clean_html_content(content)

                # å¦‚æœæ²¡æœ‰contentï¼Œä½¿ç”¨brief
                if not content and brief:
                    content = brief

                # æ—¶é—´è½¬æ¢
                if ctime and isinstance(ctime, (int, float)) and ctime > 0:
                    try:
                        # ctimeå¯èƒ½æ˜¯ç§’çº§æ—¶é—´æˆ³
                        if ctime > 1e10:  # æ¯«ç§’çº§æ—¶é—´æˆ³
                            ctime = ctime / 1000
                        publish_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')
                    except (ValueError, OSError):
                        publish_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                else:
                    publish_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                print(f"ğŸ“‹ ä»JSONæ•°æ®æå–æˆåŠŸ: æ ‡é¢˜é•¿åº¦={len(title)}, å†…å®¹é•¿åº¦={len(content)}")
                return title, publish_time, author, content

            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            except Exception as e:
                print(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")

        print("âŒ æœªæ‰¾åˆ°Next.jsæ•°æ®")
        return "", "", "è´¢è”ç¤¾", ""

    def clean_html_content(self, content):
        """æ¸…ç†HTMLå†…å®¹ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
        if not content:
            return ""

        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)

        # è§£ç HTMLå®ä½“
        content = html.unescape(content)

        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content).strip()

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[\u200b\u00a0]', ' ', content)

        return content

    def extract_from_rendered_html(self, html_content):
        """å¤‡ç”¨æ–¹æ³•ï¼šä»æ¸²æŸ“åçš„HTMLä¸­æå–ä¿¡æ¯"""
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ ‡é¢˜
            title = ""
            title_patterns = [
                r'<span[^>]*class="[^"]*detail-title-content[^"]*"[^>]*>(.*?)</span>',
                r'<h1[^>]*class="[^"]*title[^"]*"[^>]*>(.*?)</h1>',
                r'<h1[^>]*>(.*?)</h1>',
                r'<title>(.*?)</title>'
            ]

            for pattern in title_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
                if match:
                    title = self.clean_html_content(match.group(1))
                    if title and len(title) > 5:
                        break

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å†…å®¹
            content = ""
            content_patterns = [
                r'<div[^>]*class="[^"]*detail-content[^"]*"[^>]*>(.*?)</div>',
                r'<div[^>]*class="[^"]*article-content[^"]*"[^>]*>(.*?)</div>',
                r'<div[^>]*class="[^"]*content[^"]*"[^>]*>(.*?)</div>'
            ]

            for pattern in content_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
                if match:
                    content = self.clean_html_content(match.group(1))
                    if content and len(content) > 50:
                        break

            print(f"ğŸ“‹ ä»HTMLæå–: æ ‡é¢˜é•¿åº¦={len(title)}, å†…å®¹é•¿åº¦={len(content)}")
            return title, content

        except Exception as e:
            print(f"âŒ HTMLè§£æå¤±è´¥: {e}")
            return "", ""

    def extract_title_from_all_text(self, text, url):
        """ä»å…¨æ–‡ä¸­æå–æ ‡é¢˜"""
        # ç§»é™¤HTMLæ ‡ç­¾åå†å¤„ç†
        text = re.sub(r'<[^>]+>', ' ', text)
        lines = text.split('\n')

        for line in lines[:50]:  # çœ‹å‰50è¡Œ
            line = line.strip()

            if (8 <= len(line) <= 150 and
                    not line.startswith(('å…³äºæˆ‘ä»¬', 'ç½‘ç«™å£°æ˜', 'è”ç³»æ–¹å¼', 'ç”¨æˆ·åé¦ˆ',
                                         'è´¢è”ç¤¾-ä¸»æµè´¢ç»æ–°é—»é›†å›¢', 'http', 'www')) and
                    not line.endswith(('.com', '.cn')) and
                    'Â©' not in line and
                    'ç‰ˆæƒ' not in line and
                    line.count('|') < 2 and
                    any(char in line for char in ['ã€', 'ï¼š', 'ï¼Œ', 'ï¼', 'ï¼Ÿ', '"', '"']) and
                    not line.isdigit() and
                    not re.match(r'^\d+[-å¹´æœˆæ—¥æ—¶åˆ†ç§’\s:ï¼š]+', line)):

                # æ¸…ç†æ ‡é¢˜
                line = re.sub(r'^[^\u4e00-\u9fff]*', '', line)  # ç§»é™¤å¼€å¤´çš„éä¸­æ–‡å­—ç¬¦
                line = re.sub(r'[^\u4e00-\u9fff]*$', '', line)  # ç§»é™¤ç»“å°¾çš„éä¸­æ–‡å­—ç¬¦

                if len(line) >= 8:
                    return line

        # ä»URLæå–
        if '/detail/' in url:
            detail_id = url.split('/detail/')[-1]
            return f"è´¢è”ç¤¾æ–°é—»_{detail_id}"

        return f"è´¢è”ç¤¾æ–°é—»_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"

    def extract_time_from_all_text(self, text):
        """ä»å…¨æ–‡ä¸­æå–æ—¶é—´"""
        time_patterns = [
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥\s+\d{1,2}:\d{2})',
            r'(\d{4}-\d{1,2}-\d{1,2}\s+\d{1,2}:\d{2})',
            r'(\d{1,2}æœˆ\d{1,2}æ—¥\s+\d{1,2}:\d{2})',
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
            r'2025-05-\d{1,2}\s+\d{1,2}:\d{2}',
        ]

        for pattern in time_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1) if match.groups() else match.group(0)

        return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def extract_author_from_all_text(self, text):
        """ä»å…¨æ–‡ä¸­æå–ä½œè€…"""
        author_patterns = [
            r'è´¢è”ç¤¾è®°è€…\s+([^\s\n<>]+)',
            r'è´¢è”ç¤¾\s+([^\s\n<>]+)',
            r'è®°è€…\s+([^\s\n<>]+)',
            r'ç¼–è¾‘\s+([^\s\n<>]+)',
        ]

        for pattern in author_patterns:
            match = re.search(pattern, text)
            if match:
                author = match.group(1).strip()
                if author and len(author) < 10:
                    return author

        return "è´¢è”ç¤¾"

    def extract_content_from_all_text(self, text):
        """ä»å…¨æ–‡ä¸­æå–æ­£æ–‡å†…å®¹"""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '\n', text)
        lines = text.split('\n')
        content_lines = []
        content_started = False

        for line in lines:
            line = line.strip()

            # è·³è¿‡å¯¼èˆªå’Œé¡µè„š
            if (line.startswith(('å…³äºæˆ‘ä»¬', 'ç½‘ç«™å£°æ˜', 'è”ç³»æ–¹å¼', 'ç”¨æˆ·åé¦ˆ',
                                 'å¸®åŠ©', 'é¦–é¡µ', 'ç”µæŠ¥', 'è¯é¢˜', 'ç›¯ç›˜', 'VIP')) or
                    'ä¸Šè¯æŒ‡æ•°' in line or 'æ·±è¯æˆæŒ‡' in line or
                    'Â©' in line or 'ç‰ˆæƒæ‰€æœ‰' in line or
                    len(line) < 15):
                continue

            # å¯»æ‰¾æ­£æ–‡å¼€å§‹
            if ('è´¢è”ç¤¾' in line and ('æ—¥è®¯' in line or 'ç”µ' in line)) or \
                    (len(line) > 30 and ('ã€‚' in line or 'ï¼Œ' in line) and
                     not line.startswith(('http', 'www'))):
                content_started = True

            # æ”¶é›†æ­£æ–‡
            if (content_started and len(line) > 15 and
                    not line.startswith(('æ”¶è—', 'é˜…è¯»', 'æˆ‘è¦è¯„è®º', 'åé¦ˆæ„è§',
                                         'è¦é—»', 'è‚¡å¸‚', 'æŸ¥çœ‹æ›´å¤š', 'å…³è”è¯é¢˜')) and
                    not line.endswith(('.com', '.cn'))):
                content_lines.append(line)

            if len(content_lines) > 20:
                break

        result = '\n'.join(content_lines[:15])
        return result if len(result) > 30 else ""

    async def crawl(self, debug_mode=False):
        """ä¸»çˆ¬å–æ–¹æ³•"""
        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œåªè¿è¡Œè°ƒè¯•åŠŸèƒ½
        if debug_mode:
            print("ğŸ” è¿›å…¥è°ƒè¯•æ¨¡å¼...")
            await self.debug_specific_urls()
            return []

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs('data', exist_ok=True)

        # è¿æ¥NATS JetStream
        try:
            await self.connect_nats()
        except Exception as e:
            print(f"NATS JetStreamè¿æ¥å¤±è´¥ï¼Œå°†è·³è¿‡æ¶ˆæ¯å‘å¸ƒ: {e}")

        # åˆ›å»ºæµè§ˆå™¨å®ä¾‹
        async with AsyncWebCrawler(verbose=False,
                                   headless=True,
                                   always_by_pass_cache=True) as crawler:
            print("å¼€å§‹è·å–æ–°é—»é“¾æ¥...")
            link_2_category = await self.crawl_url_list(crawler=crawler)

            if not link_2_category:
                print("æœªè·å–åˆ°ä»»ä½•æ–°é—»é“¾æ¥ï¼Œé€€å‡º...")
                return []

            print(f"è·å–åˆ° {len(link_2_category)} ä¸ªæ–°é—»é“¾æ¥ï¼Œå¼€å§‹æŠ“å–è¯¦æƒ…...")

            processed_count = 0
            for link, category in link_2_category.items():
                _id = md5(link)
                if _id in self.history:
                    print(f"è·³è¿‡å·²å­˜åœ¨çš„æ–°é—»: {link}")
                    continue

                print(f"æ­£åœ¨å¤„ç†ç¬¬ {processed_count + 1} ä¸ªæ–°é—»: {link}")

                try:
                    news = await self.crawl_newsletter(link, category, crawler)

                    if not news or not news.get('title') or len(news.get('title', '').strip()) < 5:
                        print(f"æ–°é—»å†…å®¹æå–å¤±è´¥ï¼Œè·³è¿‡: {link}")
                        continue

                    print(f"ä¿å­˜æ–°é—»: {news['title']}")
                    self.save(news)

                    # å‘å¸ƒåˆ°NATS JetStream
                    await self.publish_news(news)

                    processed_count += 1

                    # æ¯å¤„ç†3ä¸ªæ–°é—»ä¼‘æ¯ä¸€ä¸‹
                    if processed_count % 3 == 0:
                        print(f"å·²å¤„ç† {processed_count} ä¸ªæ–°é—»ï¼Œæš‚åœ3ç§’...")
                        await asyncio.sleep(3)

                except Exception as e:
                    print(f'å¤„ç†æ–°é—»è¯¦æƒ…å¤±è´¥: {link} - {str(e)}')
                    continue

        # å…³é—­NATSè¿æ¥
        if self.nats_client:
            try:
                await self.nats_client.close()
                print("ğŸ”š NATS JetStreamè¿æ¥å·²å…³é—­")
            except Exception as e:
                print(f"âŒ å…³é—­NATS JetStreamè¿æ¥å¤±è´¥: {e}")

        print(f"çˆ¬å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–°é—»")
        return await self.get_last_day_data()


async def test_nats_jetstream_connection():
    """æµ‹è¯•NATS JetStreamè¿æ¥"""
    try:
        nc = NATS()
        await nc.connect("nats://localhost:4222")
        print("âœ… NATSè¿æ¥æµ‹è¯•æˆåŠŸ")

        js = nc.jetstream()

        # å°è¯•å‘å¸ƒä¸€æ¡æµ‹è¯•æ¶ˆæ¯
        test_msg = {"test": "message", "timestamp": datetime.datetime.now().isoformat()}
        ack = await js.publish("test.subject", json.dumps(test_msg).encode())
        print(f"âœ… JetStreamå‘å¸ƒæµ‹è¯•æˆåŠŸï¼Œåºå·: {ack.seq}")

        await nc.close()
        return True
    except Exception as e:
        print(f"âŒ NATS JetStreamè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == '__main__':
    import sys

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    debug_mode = '--debug' in sys.argv

    if debug_mode:
        print("ğŸ” å¯åŠ¨è°ƒè¯•æ¨¡å¼...")
        crawler = CLSCrawler()
        asyncio.run(crawler.crawl(debug_mode=True))
    else:
        # å…ˆæµ‹è¯•NATS JetStreamè¿æ¥
        asyncio.run(test_nats_jetstream_connection())

        # ç„¶åè¿è¡Œçˆ¬è™«
        asyncio.run(CLSCrawler().crawl())
